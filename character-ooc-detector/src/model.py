"""
å¤šå¤´Reward Modelæ¶æ„
åŒ…å«5ä¸ªè¯„åˆ†å¤´ï¼šstyle, values, knowledge, etiquette, stress
"""

import os
import json
import torch
import torch.nn as nn
from transformers import AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification
from typing import Dict, Optional


class MultiHeadRewardModel(PreTrainedModel):
    """
    å¤šå¤´å¥–åŠ±æ¨¡å‹
    
    æ¶æ„ï¼š
    - å…±äº«ç¼–ç å™¨ï¼ˆDeBERTa-v3-baseï¼‰
    - 4ä¸ªç‹¬ç«‹çš„è¯„åˆ†å¤´ï¼ˆæ¯ä¸ªç»´åº¦ä¸€ä¸ªï¼‰
    - æ¯ä¸ªå¤´è¾“å‡ºä¸€ä¸ªæ ‡é‡åˆ†æ•°
    """
    
    def __init__(self, config, num_dimensions: int = 4, dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            config: transformeré…ç½®
            num_dimensions: ç»´åº¦æ•°é‡ï¼ˆé»˜è®¤4ï¼šstyle, value_system, knowledge, etiquetteï¼‰
            dropout_rate: Dropoutæ¯”ç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        """
        super().__init__(config)
        
        self.num_dimensions = num_dimensions
        self.dimension_names = ["style", "value_system", "knowledge", "etiquette"]
        self.dropout_rate = dropout_rate
        
        # å…±äº«ç¼–ç å™¨
        self.encoder = AutoModel.from_config(config)
        
        # æ¯ä¸ªç»´åº¦ä¸€ä¸ªè¯„åˆ†å¤´ï¼ˆå¢å¼ºæ­£åˆ™åŒ–ï¼‰
        # æ¯ä¸ªç»´åº¦ä¸€ä¸ªè¯„åˆ†å¤´ï¼ˆç®€åŒ–ç»“æ„ä»¥åŒ¹é…é¢„è®­ç»ƒRMï¼‰
        self.score_heads = nn.ModuleDict({
            dim: nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(config.hidden_size, 1)
            )
            for dim in self.dimension_names
        })
        
        # æ€»ä½“è¯„åˆ†å¤´ï¼ˆç®€åŒ–ç»“æ„ï¼‰
        self.overall_head = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(config.hidden_size, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        return_dict: bool = True,
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            return_dict: æ˜¯å¦è¿”å›å­—å…¸
            
        Returns:
            åŒ…å«å„ç»´åº¦åˆ†æ•°çš„å­—å…¸
        """
        # ç¼–ç 
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„åˆ†æ•°
        dimension_scores = {}
        for dim_name in self.dimension_names:
            score = self.score_heads[dim_name](pooled_output).squeeze(-1)  # [batch_size]
            dimension_scores[dim_name] = score
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        overall_score = self.overall_head(pooled_output).squeeze(-1)  # [batch_size]
        
        if return_dict:
            return {
                "overall_score": overall_score,
                "dimension_scores": dimension_scores,
                "pooled_output": pooled_output
            }
        else:
            return overall_score, dimension_scores


class RewardModelForTraining(nn.Module):
    """
    è®­ç»ƒç”¨çš„Reward ModelåŒ…è£…å™¨
    æ”¯æŒåå¥½å¯¹æ¯”å­¦ä¹ 
    """
    
    def __init__(self, base_model: MultiHeadRewardModel):
        super().__init__()
        self.model = base_model
    
    def forward(
        self,
        chosen_input_ids: torch.Tensor,
        chosen_attention_mask: torch.Tensor,
        rejected_input_ids: torch.Tensor,
        rejected_attention_mask: torch.Tensor,
        violated_dimension: Optional[torch.Tensor] = None,
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—åå¥½å¯¹çš„loss
        
        Args:
            chosen_input_ids: æ­£ä¾‹è¾“å…¥
            chosen_attention_mask: æ­£ä¾‹mask
            rejected_input_ids: è´Ÿä¾‹è¾“å…¥
            rejected_attention_mask: è´Ÿä¾‹mask
            violated_dimension: è¿èƒŒçš„ç»´åº¦ç´¢å¼• [batch_size]
            
        Returns:
            åŒ…å«losså’Œåˆ†æ•°çš„å­—å…¸
        """
        # å‰å‘ä¼ æ’­æ­£ä¾‹
        chosen_outputs = self.model(
            input_ids=chosen_input_ids,
            attention_mask=chosen_attention_mask
        )
        
        # å‰å‘ä¼ æ’­è´Ÿä¾‹
        rejected_outputs = self.model(
            input_ids=rejected_input_ids,
            attention_mask=rejected_attention_mask
        )
        
        # è®¡ç®—ranking loss (chosenåº”è¯¥æ¯”rejectedåˆ†æ•°é«˜)
        losses = {}
        
        # 1. æ€»ä½“ranking loss
        overall_loss = -torch.log(
            torch.sigmoid(chosen_outputs["overall_score"] - rejected_outputs["overall_score"])
        ).mean()
        losses["overall_loss"] = overall_loss
        
        # 2. å„ç»´åº¦çš„ranking loss
        dimension_losses = []
        for dim_name in self.model.dimension_names:
            chosen_score = chosen_outputs["dimension_scores"][dim_name]
            rejected_score = rejected_outputs["dimension_scores"][dim_name]
            
            dim_loss = -torch.log(
                torch.sigmoid(chosen_score - rejected_score)
            ).mean()
            
            losses[f"{dim_name}_loss"] = dim_loss
            dimension_losses.append(dim_loss)
        
        # 3. ç»´åº¦åŠ æƒlossï¼ˆå¦‚æœæä¾›äº†violated_dimensionï¼‰
        if violated_dimension is not None:
            # å¯¹è¿èƒŒçš„ç»´åº¦ç»™äºˆæ›´é«˜æƒé‡
            weighted_dim_loss = 0
            for i, dim_name in enumerate(self.model.dimension_names):
                weight = torch.where(violated_dimension == i, 2.0, 1.0).float()
                dim_loss = -torch.log(
                    torch.sigmoid(
                        chosen_outputs["dimension_scores"][dim_name] - 
                        rejected_outputs["dimension_scores"][dim_name]
                    )
                )
                weighted_dim_loss += (dim_loss * weight).mean()
            
            losses["weighted_dimension_loss"] = weighted_dim_loss / len(self.model.dimension_names)
        
        # æ€»loss
        total_loss = overall_loss + sum(dimension_losses) / len(dimension_losses)
        if violated_dimension is not None:
            total_loss = 0.5 * total_loss + 0.5 * losses["weighted_dimension_loss"]
        
        losses["loss"] = total_loss
        
        return {
            **losses,
            "chosen_scores": chosen_outputs,
            "rejected_scores": rejected_outputs
        }


def create_reward_model(
    model_name: str = "microsoft/deberta-v3-base",
    pretrained_rm_path: Optional[str] = None
) -> MultiHeadRewardModel:
    """
    åˆ›å»ºå¤šå¤´Reward Model
    
    Args:
        model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
        pretrained_rm_path: é¢„è®­ç»ƒRMçš„è·¯å¾„ (å¯é€‰, ç”¨äºå¾®è°ƒ)
        
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å‹
    """
    config = AutoConfig.from_pretrained(pretrained_rm_path or model_name)
    model = MultiHeadRewardModel(config)
    
    if pretrained_rm_path:
        print(f"ğŸ”„ ä»é¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ '{pretrained_rm_path}' åŠ è½½æƒé‡è¿›è¡Œå¾®è°ƒ...")
        
        # åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒRM
        try:
            pretrained_rm = AutoModelForSequenceClassification.from_pretrained(pretrained_rm_path)
        except Exception as e:
            print(f"âŒ åŠ è½½é¢„è®­ç»ƒRM '{pretrained_rm_path}' å¤±è´¥: {e}")
            print("    å°†å›é€€åˆ°ä»åŸºç¡€æ¨¡å‹åˆå§‹åŒ–ã€‚")
            pretrained = AutoModel.from_pretrained(model_name)
            model.encoder = pretrained
            return model

        # 1. å¤åˆ¶encoderæƒé‡
        #    ä¸åŒæ¨¡å‹çš„base modelå±æ€§åä¸åŒ (e.g., 'deberta', 'roberta', 'base_model')
        encoder_loaded = False
        for attr in ['deberta', 'roberta', 'base_model']:
            if hasattr(pretrained_rm, attr):
                model.encoder = getattr(pretrained_rm, attr)
                encoder_loaded = True
                print(f"    - âœ“ æˆåŠŸå¤åˆ¶ '{attr}' çš„ç¼–ç å™¨æƒé‡ã€‚")
                break
        if not encoder_loaded:
            print("    - âš ï¸  è­¦å‘Š: æ— æ³•è‡ªåŠ¨ç¡®å®šencoderï¼Œè·³è¿‡encoderæƒé‡åŠ è½½ã€‚")

        # 2. å°†é¢„è®­ç»ƒRMçš„åˆ†ç±»å¤´æƒé‡å¤åˆ¶åˆ°æˆ‘ä»¬æ‰€æœ‰çš„å¤´ä¸Š
        if hasattr(pretrained_rm, "classifier") and isinstance(pretrained_rm.classifier, nn.Linear):
            rm_head_state_dict = pretrained_rm.classifier.state_dict()
            
            for head in model.score_heads.values():
                # head[1] is nn.Linear
                head[1].load_state_dict(rm_head_state_dict)
            
            model.overall_head[1].load_state_dict(rm_head_state_dict)
            print("    - âœ“ æˆåŠŸå°†é¢„è®­ç»ƒRMçš„è¯„åˆ†å¤´æƒé‡å¤åˆ¶åˆ°æ‰€æœ‰å¤´ã€‚")
        else:
            print("    - âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°å…¼å®¹çš„è¯„åˆ†å¤´ (nn.Linear)ï¼Œè¯„åˆ†å¤´å°†éšæœºåˆå§‹åŒ–ã€‚")
        
        print("    - æƒé‡åŠ è½½å®Œæˆã€‚")

    else:
        print(f"ğŸ”„ ä»åŸºç¡€æ¨¡å‹ '{model_name}' åˆå§‹åŒ–æƒé‡...")
        # åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°encoder
        pretrained = AutoModel.from_pretrained(model_name)
        model.encoder = pretrained
    
    return model


def load_reward_model(checkpoint_path: str) -> MultiHeadRewardModel:
    """
    ä»checkpointåŠ è½½æ¨¡å‹
    
    Args:
        checkpoint_path: checkpointè·¯å¾„
        
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    # é¦–å…ˆåŠ è½½é…ç½®
    config = AutoConfig.from_pretrained(checkpoint_path)
    
    # åŠ¨æ€ç¡®å®šç»´åº¦æ•°é‡
    training_info_path = os.path.join(checkpoint_path, "training_info.json")
    if os.path.exists(training_info_path):
        with open(training_info_path, 'r') as f:
            info = json.load(f)
            num_dimensions = info.get("num_dimensions", 4) # é»˜è®¤ä¸º4
    else:
        num_dimensions = 4
        
    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    model = MultiHeadRewardModel(config, num_dimensions=num_dimensions)
    
    try:
        state_dict = torch.load(
            os.path.join(checkpoint_path, "pytorch_model.bin"),
            map_location="cpu"
        )
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        print(f"âš ï¸  åŠ è½½æ¨¡å‹æƒé‡æ—¶å‡ºç°ä¸åŒ¹é…: {e}")
        print("    å°è¯•ä»¥éä¸¥æ ¼æ¨¡å¼åŠ è½½...")
        model.load_state_dict(state_dict, strict=False)

    return model


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = create_reward_model()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 128
    
    input_ids = torch.randint(0, 30000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    
    print(f"Overall score shape: {outputs['overall_score'].shape}")
    print(f"Dimension scores:")
    for dim, score in outputs['dimension_scores'].items():
        print(f"  {dim}: {score.shape}")
    
    # æµ‹è¯•è®­ç»ƒåŒ…è£…å™¨
    print("\næµ‹è¯•è®­ç»ƒæ¨¡å¼...")
    training_model = RewardModelForTraining(model)
    
    chosen_ids = torch.randint(0, 30000, (batch_size, seq_len))
    rejected_ids = torch.randint(0, 30000, (batch_size, seq_len))
    chosen_mask = torch.ones(batch_size, seq_len)
    rejected_mask = torch.ones(batch_size, seq_len)
    violated_dim = torch.tensor([0, 1])  # style, values
    
    train_outputs = training_model(
        chosen_input_ids=chosen_ids,
        chosen_attention_mask=chosen_mask,
        rejected_input_ids=rejected_ids,
        rejected_attention_mask=rejected_mask,
        violated_dimension=violated_dim
    )
    
    print(f"Total loss: {train_outputs['loss'].item():.4f}")
    print(f"Overall loss: {train_outputs['overall_loss'].item():.4f}")
    for dim in model.dimension_names:
        print(f"{dim} loss: {train_outputs[f'{dim}_loss'].item():.4f}")
    
    print("\næ¨¡å‹æµ‹è¯•å®Œæˆï¼")