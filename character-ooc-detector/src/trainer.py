"""
Reward Modelè®­ç»ƒå™¨
æ”¯æŒåå¥½å¯¹è®­ç»ƒå’Œå¤šç»´åº¦è¯„åˆ†
"""

import os
import json
import random
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, get_linear_schedule_with_warmup, set_seed
from tqdm import tqdm
from typing import List, Dict, Optional

from .model import MultiHeadRewardModel, RewardModelForTraining, create_reward_model
from .sample_generator import PreferencePair


class PreferencePairDataset(Dataset):
    """åå¥½å¯¹æ•°æ®é›†"""
    
    def __init__(
        self,
        pairs: List[PreferencePair],
        tokenizer,
        max_length: int = 512
    ):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ç»´åº¦ååˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç§»é™¤æ— æ•ˆçš„stressç»´åº¦ï¼‰
        self.dim_to_idx = {
            "style": 0,
            "value_system": 1,
            "knowledge": 2,
            "etiquette": 3
        }
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        pair = self.pairs[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬ï¼šä¸Šä¸‹æ–‡ + å›å¤
        chosen_text = f"{pair.context}\nå›å¤: {pair.chosen}"
        rejected_text = f"{pair.context}\nå›å¤: {pair.rejected}"
        
        # Tokenize
        chosen_encodings = self.tokenizer(
            chosen_text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        rejected_encodings = self.tokenizer(
            rejected_text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        # è¿èƒŒçš„ç»´åº¦
        violated_dim_idx = self.dim_to_idx.get(pair.violated_dimension, 0)
        
        return {
            "chosen_input_ids": chosen_encodings["input_ids"].squeeze(0),
            "chosen_attention_mask": chosen_encodings["attention_mask"].squeeze(0),
            "rejected_input_ids": rejected_encodings["input_ids"].squeeze(0),
            "rejected_attention_mask": rejected_encodings["attention_mask"].squeeze(0),
            "violated_dimension": torch.tensor(violated_dim_idx, dtype=torch.long)
        }


class RMTrainer:
    """Reward Modelè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "microsoft/deberta-v3-base",
        output_dir: str = "./checkpoints",
        device: str = None,
        pretrained_rm_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
            pretrained_rm_path: é¢„è®­ç»ƒRMè·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºå¾®è°ƒï¼‰
        """
        self.model_name = model_name
        self.pretrained_rm_path = pretrained_rm_path
        self.output_dir = output_dir
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def train(
        self,
        train_pairs: List[PreferencePair],
        val_pairs: Optional[List[PreferencePair]] = None,
        num_epochs: int = 10,
        batch_size: int = 6,
        learning_rate: float = 2e-5,
        warmup_steps: int = 100,
        max_length: int = 512,
        save_steps: int = 500,
        eval_steps: int = 500,
        use_bf16: bool = True,
        seed: Optional[int] = 42,
        weight_decay: float = 0.01,
        early_stopping_patience: int = 3
    ):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_pairs: è®­ç»ƒåå¥½å¯¹
            val_pairs: éªŒè¯åå¥½å¯¹ï¼ˆå¯é€‰ï¼‰
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            warmup_steps: warmupæ­¥æ•°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            save_steps: ä¿å­˜é—´éš”
            eval_steps: è¯„ä¼°é—´éš”
            use_bf16: æ˜¯å¦ä½¿ç”¨bf16æ··åˆç²¾åº¦ï¼ˆé»˜è®¤Trueï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
            seed: éšæœºç§å­ï¼ˆé»˜è®¤42ï¼Œè®¾ä¸ºNoneç¦ç”¨ï¼‰
            weight_decay: L2æ­£åˆ™åŒ–ç³»æ•°ï¼ˆé»˜è®¤0.01ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            early_stopping_patience: æ—©åœpatienceï¼ˆéªŒè¯lossä¸ä¸‹é™çš„epochæ•°ï¼‰
        """
        # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
        if seed is not None:
            print(f"\nğŸ² è®¾ç½®éšæœºç§å­: {seed}")
            set_seed(seed)
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
            # è®¾ç½®ç¡®å®šæ€§ç®—æ³•ï¼ˆå¯èƒ½ç•¥å¾®å½±å“æ€§èƒ½ï¼‰
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        
        print(f"\nå¼€å§‹è®­ç»ƒ...")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_pairs)}")
        if val_pairs:
            print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_pairs)}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = PreferencePairDataset(train_pairs, self.tokenizer, max_length)
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0
        )
        
        val_loader = None
        if val_pairs:
            val_dataset = PreferencePairDataset(val_pairs, self.tokenizer, max_length)
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0
            )
        
        # åˆ›å»ºæ¨¡å‹
        base_model = create_reward_model(
            model_name=self.model_name,
            pretrained_rm_path=self.pretrained_rm_path
        )
        model = RewardModelForTraining(base_model)
        
        # æ··åˆç²¾åº¦è®¾ç½®
        if use_bf16 and self.device == "cuda":
            if torch.cuda.is_bf16_supported():
                print("âœ“ ä½¿ç”¨BF16æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
                model = model.to(self.device, dtype=torch.bfloat16)
                use_amp = True
                amp_dtype = torch.bfloat16
            else:
                print("âš ï¸  å½“å‰GPUä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP32")
                model = model.to(self.device)
                use_amp = False
                amp_dtype = torch.float32
        else:
            model = model.to(self.device)
            use_amp = False
            amp_dtype = torch.float32
        
        # æ˜¾ç¤ºæ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # ä¼°ç®—æ˜¾å­˜å ç”¨ï¼ˆç²—ç•¥ï¼‰
        bytes_per_param = 2 if use_amp else 4  # bf16: 2 bytes, fp32: 4 bytes
        model_size_mb = (total_params * bytes_per_param) / (1024**2)
        
        print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  åŸºç¡€æ¨¡å‹: {self.model_name}")
        print(f"  æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.1f}M)")
        print(f"  ç²¾åº¦: {'BF16' if use_amp else 'FP32'}")
        print(f"  é¢„ä¼°æ˜¾å­˜: ~{model_size_mb:.0f}MB (ä»…æ¨¡å‹æƒé‡)")
        print(f"  è®¾å¤‡: {self.device}")
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆæ·»åŠ weight decayæ­£åˆ™åŒ–ï¼‰
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  Weight decay (L2): {weight_decay}")
        print(f"  Early stopping patience: {early_stopping_patience}")
        
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # è®­ç»ƒå¾ªç¯ï¼ˆæ·»åŠ early stoppingï¼‰
        global_step = 0
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(num_epochs):
            print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
            
            # è®­ç»ƒ
            model.train()
            train_loss = 0
            train_metrics = {
                "overall_loss": 0,
                "style_loss": 0,
                "value_system_loss": 0,
                "knowledge_loss": 0,
                "etiquette_loss": 0
            }
            
            progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")
            for batch in progress_bar:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # æ··åˆç²¾åº¦è®­ç»ƒ
                if use_amp:
                    with torch.autocast(device_type='cuda', dtype=amp_dtype):
                        outputs = model(**batch)
                        loss = outputs["loss"]
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                else:
                    # æ­£å¸¸ç²¾åº¦è®­ç»ƒ
                    outputs = model(**batch)
                    loss = outputs["loss"]
                    
                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                
                # è®°å½•æŒ‡æ ‡
                train_loss += loss.item()
                for key in train_metrics.keys():
                    if key in outputs:
                        train_metrics[key] += outputs[key].item()
                
                global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
                
                # å®šæœŸè¯„ä¼°
                if val_loader and global_step % eval_steps == 0:
                    val_loss, val_metrics = self._evaluate(model, val_loader)
                    print(f"\néªŒè¯ @ step {global_step}:")
                    print(f"  éªŒè¯loss: {val_loss:.4f}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        self._save_model(model.model, "best_model")
                        print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {val_loss:.4f})")
                    
                    model.train()
                
                # å®šæœŸä¿å­˜
                if global_step % save_steps == 0:
                    self._save_model(model.model, f"checkpoint-{global_step}")
            
            # Epochç»“æŸç»Ÿè®¡
            avg_train_loss = train_loss / len(train_loader)
            print(f"\nè®­ç»ƒloss: {avg_train_loss:.4f}")
            
            for key in train_metrics:
                avg_metric = train_metrics[key] / len(train_loader)
                print(f"  {key}: {avg_metric:.4f}")
            
            # Epochç»“æŸæ—¶è¯„ä¼°ï¼ˆæ·»åŠ early stoppingï¼‰
            if val_loader:
                val_loss, val_metrics = self._evaluate(model, val_loader)
                print(f"\nEpoch {epoch + 1} éªŒè¯ç»“æœ:")
                print(f"  æ€»ä½“loss: {val_loss:.4f}")
                for key, value in val_metrics.items():
                    print(f"  {key}: {value:.4f}")
                
                # Early stoppingæ£€æŸ¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    print(f"  âœ“ æ–°çš„æœ€ä½³éªŒè¯loss: {val_loss:.4f}")
                else:
                    patience_counter += 1
                    print(f"  âš ï¸  éªŒè¯lossæœªæ”¹å–„ ({patience_counter}/{early_stopping_patience})")
                    
                    if patience_counter >= early_stopping_patience:
                        print(f"\nâ¹  Early stopping triggered after {epoch + 1} epochs")
                        print(f"  æœ€ä½³éªŒè¯loss: {best_val_loss:.4f}")
                        break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model(model.model, "final_model")
        print(f"\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {self.output_dir}")
    
    def _evaluate(self, model, val_loader):
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        total_loss = 0
        metrics = {
            "overall_loss": 0,
            "style_loss": 0,
            "value_system_loss": 0,
            "knowledge_loss": 0,
            "etiquette_loss": 0,
            "stress_loss": 0
        }
        
        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}
                outputs = model(**batch)
                
                total_loss += outputs["loss"].item()
                for key in metrics.keys():
                    if key in outputs:
                        metrics[key] += outputs[key].item()
        
        avg_loss = total_loss / len(val_loader)
        for key in metrics:
            metrics[key] /= len(val_loader)
        
        return avg_loss, metrics
    
    def _save_model(self, model: MultiHeadRewardModel, name: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = os.path.join(self.output_dir, name)
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(model.state_dict(), os.path.join(save_dir, "pytorch_model.bin"))
        
        # ä¿å­˜é…ç½®
        model.config.save_pretrained(save_dir)
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(save_dir)
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        info = {
            "model_name": self.model_name,
            "dimension_names": model.dimension_names,
            "num_dimensions": model.num_dimensions
        }
        with open(os.path.join(save_dir, "training_info.json"), 'w') as f:
            json.dump(info, f, indent=2)


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒå™¨
    from sample_generator import PreferencePair
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
    dummy_pairs = [
        PreferencePair(
            context="ä½ æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ",
            chosen="è¿˜è¡Œï¼Œåœ¨ç»ƒæ–°æ‹›å¼ã€‚",
            rejected="è¶…çº§æ£’ï¼æˆ‘æœ€è¿‘å­¦äº†å¥½å¤šç°ä»£ç§‘æŠ€çŸ¥è¯†å‘¢ï¼",
            violated_dimension="knowledge"
        )
        for _ in range(50)
    ]
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RMTrainer(
        model_name="microsoft/deberta-v3-base",
        output_dir="./test_checkpoints"
        # pretrained_rm_path="OpenAssistant/reward-model-deberta-v3-base" # å¯é€‰æµ‹è¯•
    )
    
    # è®­ç»ƒ
    print("\nå¼€å§‹æµ‹è¯•è®­ç»ƒ...")
    trainer.train(
        train_pairs=dummy_pairs[:40],
        val_pairs=dummy_pairs[40:],
        num_epochs=1,
        batch_size=4,
        save_steps=20,
        eval_steps=20
    )
    
    print("\nè®­ç»ƒå™¨æµ‹è¯•å®Œæˆï¼")