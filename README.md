# Character OOC Detector (角色人格一致性检测器)

**[English Version](README_EN.md)**

---

Character-OOC-Detector 是一个基于多头奖励模型（Multi-Head Reward Model）的实验性工具，用于检测 AI 在角色扮演（Role-Playing）中生成的对话是否偏离其预设人格（Out-of-Character, OOC）。

本项目旨在探索通过微调奖励模型来对齐 AI 角色行为的方法。其核心思路是，通过对比学习“符合人设”与“违背人设”的样本对，训练一个能够从多个维度（如语言风格、价值观等）评估对话一致性的模型。
LLM在长上下文RP中时常因矛盾或演化动态复杂的上下文，冗杂的背景导致的注意力噪声均可能导致多种类型的OOC行为：知识越界，风格漂移等。这类问题可以通过Reflection Agent自我反思循环工作，或者LLM as a judge等方式评估，但如果通过一个解耦的，轻量且参数化的方式，可能得到更好的效果，并具有更多有潜力的扩展用途，核心思想是：角色本身的参数化。较为简单的首个应用即是角色偏好与决策边界的参数化建模。

早年来Persona-Chat系列与后续“对话NLI/一致性检测”工作，用NLI/对比训练判断回复是否与给定persona矛盾（例如Welleck等关于对话自相矛盾/与persona矛盾的检测）等条件化判别方案已经证明了LLM外系统判别OOC的可行性，但不覆盖风格、动机、关系礼仪、情绪轨迹等，也没有参数化。近两年出现了一些LLM as a judge的“角色扮演/设定遵循”的评测基准与报告，但大都没有参数化方案，也没有整合到写作流程中去。

## 核心概念

1.  **角色定义 (Persona)**: 使用 `.json` 文件将角色的性格特质结构化为多维度标签。
2.  **样本生成**: 基于 `Persona` 定义，生成用于训练的偏好对（chosen vs rejected）。此步骤可由 LLM (如 Gemini-2.5-pro) 辅助完成。
3.  **模型训练**: 使用偏好对数据来微调一个多头奖励模型。
4.  **OOC 评分**: 使用训练好的模型来评估新对话，通过其输出的总分和各维度分来判断其 OOC 程度。

## 主要功能

- **多头奖励模型**: 包含一个总体评分头和四个独立的维度评分头（`style`, `values`, `knowledge`, `etiquette`），用于细粒度地分析OOC原因。
- **支持从预训练RM微调**: 能够加载一个公开的、预训练好的奖励模型（如 `OpenAssistant/reward-model-deberta-v3-base`）作为起点进行微调，以提升训练稳定性和效果。
- **包含开箱即用的示例**: 本仓库提供了一套预训练好的模型 (`checkpoints/`) 和对应的数据集 (`data/`)，无需训练即可直接观察示例。

## 安装

**环境**:
- Python 3.8+
- PyTorch 2.0+
- CUDA (推荐)

**依赖**:
```bash
git clone https://github.com/your-username/character-ooc-detector.git
cd character-ooc-detector
pip install -r requirements.txt
```

## 使用指南

### 1. 测试预训练模型 (推荐)

直接运行 `test_model.py` 脚本来体验仓库中自带的预训练模型。
```bash
python test_model.py
```
该脚本会自动加载 `checkpoints/` 目录下的模型，并提供多种测试模式：
- **预设场景**: 在固定的测试用例上评估模型表现。
- **交互模式**: 自行输入上下文和角色回复，实时获取OOC评分。
- **批量测试**: 从文件加载测试用例进行批量评分。

### 2. 完整流程 (可选)

如果需要体验包括数据生成和模型训练在内的完整流程，可运行 `quick_start.py`。
```bash
python quick_start.py
```
此脚本会提供一个交互式向导，让你选择角色、管理数据、并（可选地）执行训练。训练步骤0是推荐的微調模式。

### 3. 数据管理

运行 `manage_data.py` 来查看数据统计或生成新的训练样本（需要配置API Key）。

## 配置 (可选)

仅当需要使用LLM辅助功能（如生成新样本）时，才需要配置`config/config.yaml`中的OpenAI API密钥。详细说明请参考 `CONFIG_GUIDE.md`。

## 技术架构

```
      输入: [上下文 + 回复]
             ↓
[   共享的Transformer编码器   ]
(DeBERTa, RoBERTa, etc.)
             ↓
      [CLS] Token表示
             ↓
┌────────────┴────────────┐
↓            ↓            ↓
[总分头]  [风格头] ... [礼仪头]
(Linear)   (Linear)     (Linear)
   ↓          ↓            ↓
 总分        风格分        礼仪分
```

## 测试
使用183.8M的 reward-model-deberta-v3-base作为与训练RM，在150个偏好样本上以bsz=4训练了3个epoch，时间在40s左右，测试了 6 个样本，可以看到经过简单的微调，模型已经能判别常规的例子，尽管现在的训练模式不鼓励各个头进行分化，仍是预训练模型的模式:

──────────────────────────────────────────────────────────────────────
测试 1: ✅ 符合人设
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人回到家中
回复: 欢迎回家，主人~艾莉娅已经准备好晚餐了喵！

总体评分: 1.768
是否通过: ✓ 是

各维度评分:
  style       :  1.768 ✓
  value_system:  1.772 ✓
  knowledge   :  1.764 ✓
  etiquette   :  1.771 ✓

最弱维度: knowledge (1.764)

──────────────────────────────────────────────────────────────────────
测试 2: ❌ 违背风格+知识+价值观
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人回到家中
回复: 哟，回来了？我正在看手机呢，晚饭你自己热一下。

总体评分: -4.319
是否通过: ✗ 否

各维度评分:
  style       : -4.312 ✗
  value_system: -4.311 ✗
  knowledge   : -4.311 ✗
  etiquette   : -4.313 ✗

最弱维度: etiquette (-4.313)

──────────────────────────────────────────────────────────────────────
测试 3: ✅ 符合人设
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人问：'能帮我泡杯茶吗？'
回复: 当然，艾莉娅这就去准备红茶呢，请主人稍候喵~

总体评分: 1.372
是否通过: ✓ 是

各维度评分:
  style       :  1.372 ✓
  value_system:  1.376 ✓
  knowledge   :  1.367 ✓
  etiquette   :  1.374 ✓

最弱维度: knowledge (1.367)

──────────────────────────────────────────────────────────────────────
测试 4: ❌ 违背知识边界
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人问：'能帮我泡杯茶吗？'
回复: 好的，我去用咖啡机给你冲一杯速溶咖啡。

总体评分: -3.995
是否通过: ✗ 否

各维度评分:
  style       : -3.990 ✗
  value_system: -3.989 ✗
  knowledge   : -3.989 ✗
  etiquette   : -3.990 ✗

最弱维度: etiquette (-3.990)

──────────────────────────────────────────────────────────────────────
测试 5: ✅ 符合人设
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人身体不适
回复: 主人！艾莉娅这就去准备热水和毛巾，请您好好休息喵~

总体评分: 2.126
是否通过: ✓ 是

各维度评分:
  style       :  2.125 ✓
  value_system:  2.129 ✓
  knowledge   :  2.120 ✓
  etiquette   :  2.127 ✓

最弱维度: knowledge (2.120)

──────────────────────────────────────────────────────────────────────
测试 6: ❌ 违背价值观
──────────────────────────────────────────────────────────────────────
角色: 艾莉娅

上下文: 主人身体不适
回复: 那你自己去看医生吧，我还有事要做。

总体评分: -4.308
是否通过: ✗ 否

各维度评分:
  style       : -4.302 ✗
  value_system: -4.302 ✗
  knowledge   : -4.300 ✗
  etiquette   : -4.303 ✗

最弱维度: etiquette (-4.303)

## 未来工作

这是一个研究性的MVP项目，未来可探索的方向包括：
- **证据定位**: 让模型指出是哪个具体文本片段导致了OOC。
- **元检测器**: 训练一个能同时理解多个角色的通用OOC检测模型。
- **联合训练**: 与主LLM的特征进行耦合并直接参加到生成回路。

## 许可证

MIT License